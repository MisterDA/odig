<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml"><head><title>Async_unix__Scheduler (async_unix.Async_unix__Scheduler)</title><link rel="stylesheet" href="../../_odoc-theme/odoc.css"/><meta charset="utf-8"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><script src="../../highlight.pack.js"></script><script>hljs.initHighlightingOnLoad();</script></head><body><div class="content"><header><nav><a href="../index.html">Up</a> – <a href="../index.html">async_unix</a> &#x00BB; Async_unix__Scheduler</nav><h1>Module <code>Async_unix__Scheduler</code></h1><p>Dispatches and monitors Async processes.</p><p>The threading model is as follows. Only one thread runs Async code at a time. This is enforced by a single lock in Async's scheduler data structure. There are any number of threads running code without holding the lock that get data from the outside world and want to affect the Async world. They do this by calling <code>Thread_safe.run_in_async*</code>, which acquires the lock, does a computation (e.g., fills an ivar), and then runs a &quot;cycle&quot; of Async computations.</p></header><dl><dt class="spec type" id="type-t"><a href="#type-t" class="anchor"></a><code><span class="keyword">type</span> t</code><code> = <a href="../Async_unix__/Raw_scheduler/index.html#type-t">Async_unix__.Raw_scheduler.t</a></code></dt></dl><div><div class="spec include"><div class="doc"><details open="open"><summary><span class="def"><code><span class="keyword">include</span> <span class="keyword">sig</span> ... <span class="keyword">end</span></code></span></summary><dl><dt class="spec value" id="val-sexp_of_t"><a href="#val-sexp_of_t" class="anchor"></a><code><span class="keyword">val</span> sexp_of_t : <a href="index.html#type-t">t</a> <span>&#45;&gt;</span> <a href="../../sexplib0/Sexplib0/Sexp/index.html#type-t">Ppx_sexp_conv_lib.Sexp.t</a></code></dt></dl></details></div></div></div><dl><dt class="spec value" id="val-t"><a href="#val-t" class="anchor"></a><code><span class="keyword">val</span> t : unit <span>&#45;&gt;</span> <a href="index.html#type-t">t</a></code></dt><dd><p><code>t ()</code> returns the Async scheduler. If the scheduler hasn't been created yet, this will create it and acquire the Async lock.</p></dd></dl><aside><p>Accessors</p></aside><dl><dt class="spec value" id="val-max_num_open_file_descrs"><a href="#val-max_num_open_file_descrs" class="anchor"></a><code><span class="keyword">val</span> max_num_open_file_descrs : unit <span>&#45;&gt;</span> int</code></dt><dt class="spec value" id="val-max_num_threads"><a href="#val-max_num_threads" class="anchor"></a><code><span class="keyword">val</span> max_num_threads : unit <span>&#45;&gt;</span> int</code></dt><dt class="spec value" id="val-go"><a href="#val-go" class="anchor"></a><code><span class="keyword">val</span> go : ?&#8288;raise_unhandled_exn:bool <span>&#45;&gt;</span> unit <span>&#45;&gt;</span> <a href="../../core/Core/index.html#type-never_returns">Core.never_returns</a></code></dt><dd><p><code>go ?raise_unhandled_exn ()</code> passes control to Async, at which point Async starts running handlers, one by one without interruption, until there are no more handlers to run. When Async is out of handlers, it blocks until the outside world schedules more of them. Because of this, Async programs do not exit until <code>shutdown</code> is called.</p><p><code>go ()</code> calls <code>handle_signal Sys.sigpipe</code>, which causes the SIGPIPE signal to be ignored. Low-level syscalls (e.g., write) still raise EPIPE.</p><p>If any Async job raises an unhandled exception that is not handled by any monitor, Async execution ceases. Then, by default, Async pretty prints the exception, and exits with status 1. If you don't want this, pass <code>~raise_unhandled_exn:true</code>, which will cause the unhandled exception to be raised to the caller of <code>go ()</code>.</p></dd></dl><dl><dt class="spec value" id="val-go_main"><a href="#val-go_main" class="anchor"></a><code><span class="keyword">val</span> go_main : ?&#8288;raise_unhandled_exn:bool <span>&#45;&gt;</span> ?&#8288;file_descr_watcher:<a href="../../async_kernel/Async_kernel__Async_kernel_config/File_descr_watcher/index.html#type-t">Async_unix__.Config.File_descr_watcher.t</a> <span>&#45;&gt;</span> ?&#8288;max_num_open_file_descrs:int <span>&#45;&gt;</span> ?&#8288;max_num_threads:int <span>&#45;&gt;</span> main:(unit <span>&#45;&gt;</span> unit) <span>&#45;&gt;</span> unit <span>&#45;&gt;</span> <a href="../../core/Core/index.html#type-never_returns">Core.never_returns</a></code></dt><dd><p><code>go_main</code> is like <code>go</code>, except that you supply a <code>main</code> function that will be run to initialize the Async computation, and that <code>go_main</code> will fail if any Async has been used prior to <code>go_main</code> being called. Moreover it allows you to configure more static options of the scheduler.</p></dd></dl><dl><dt class="spec type" id="type-with_options"><a href="#type-with_options" class="anchor"></a><code><span class="keyword">type</span> 'a with_options</code><code> = ?&#8288;monitor:<a href="../../async_kernel/Async_kernel/Monitor/index.html#type-t">Async_unix__.Import.Monitor.t</a> <span>&#45;&gt;</span> ?&#8288;priority:<a href="../../async_kernel/Async_kernel/Priority/index.html#type-t">Async_unix__.Import.Priority.t</a> <span>&#45;&gt;</span> <span class="type-var">'a</span></code></dt></dl><dl><dt class="spec value" id="val-current_execution_context"><a href="#val-current_execution_context" class="anchor"></a><code><span class="keyword">val</span> current_execution_context : unit <span>&#45;&gt;</span> <a href="../../async_kernel/Async_kernel/Execution_context/index.html#type-t">Async_unix__.Import.Execution_context.t</a></code></dt><dt class="spec value" id="val-within_context"><a href="#val-within_context" class="anchor"></a><code><span class="keyword">val</span> within_context : <a href="../../async_kernel/Async_kernel/Execution_context/index.html#type-t">Async_unix__.Import.Execution_context.t</a> <span>&#45;&gt;</span> (unit <span>&#45;&gt;</span> <span class="type-var">'a</span>) <span>&#45;&gt;</span> (<span class="type-var">'a</span>, unit) <a href="../../core_kernel/Core_kernel/Result/index.html#type-t">Core.Result.t</a></code></dt><dd><p><code>within_context context f</code> runs <code>f ()</code> right now with the specified execution context. If <code>f</code> raises, then the exception is sent to the monitor of <code>context</code>, and <code>Error ()</code> is returned.</p></dd></dl><dl><dt class="spec value" id="val-within'"><a href="#val-within'" class="anchor"></a><code><span class="keyword">val</span> within' : ((unit <span>&#45;&gt;</span> <span class="type-var">'a</span> <a href="../../async_kernel/Async_kernel/Deferred/index.html#type-t">Async_unix__.Import.Deferred.t</a>) <span>&#45;&gt;</span> <span class="type-var">'a</span> <a href="../../async_kernel/Async_kernel/Deferred/index.html#type-t">Async_unix__.Import.Deferred.t</a>) <a href="index.html#type-with_options">with_options</a></code></dt><dd><p><code>within' f ~monitor ~priority</code> runs <code>f ()</code> right now, with the specified block group, monitor, and priority set as specified. They will be reset to their original values when <code>f</code> returns. If <code>f</code> raises, then the result of <code>within'</code> will never become determined, but the exception will end up in the specified monitor.</p></dd></dl><dl><dt class="spec value" id="val-within"><a href="#val-within" class="anchor"></a><code><span class="keyword">val</span> within : ((unit <span>&#45;&gt;</span> unit) <span>&#45;&gt;</span> unit) <a href="index.html#type-with_options">with_options</a></code></dt><dd><p><code>within</code> is like <code>within'</code>, but doesn't require the thunk to return a deferred.</p></dd></dl><dl><dt class="spec value" id="val-within_v"><a href="#val-within_v" class="anchor"></a><code><span class="keyword">val</span> within_v : ((unit <span>&#45;&gt;</span> <span class="type-var">'a</span>) <span>&#45;&gt;</span> <span class="type-var">'a</span> option) <a href="index.html#type-with_options">with_options</a></code></dt><dd><p><code>within_v</code> is like <code>within</code>, but allows a value to be returned by <code>f</code>.</p></dd></dl><dl><dt class="spec value" id="val-with_local"><a href="#val-with_local" class="anchor"></a><code><span class="keyword">val</span> with_local : <span class="type-var">'a</span> <a href="../../core_kernel/Core_kernel__Type_equal/Id/index.html#type-t">Core.Univ_map.Key.t</a> <span>&#45;&gt;</span> <span class="type-var">'a</span> option <span>&#45;&gt;</span> f:(unit <span>&#45;&gt;</span> <span class="type-var">'b</span>) <span>&#45;&gt;</span> <span class="type-var">'b</span></code></dt><dd><p><code>with_local key value ~f</code>, when run in the current execution context, <code>e</code>, runs <code>f</code> right now in a new execution context, <code>e'</code>, that is identical to <code>e</code> except that <code>find_local key = value</code>. As usual, <code>e'</code> will be in effect in asynchronous computations started by <code>f</code>. When <code>with_local</code> returns, the execution context is restored to <code>e</code>.</p></dd></dl><dl><dt class="spec value" id="val-find_local"><a href="#val-find_local" class="anchor"></a><code><span class="keyword">val</span> find_local : <span class="type-var">'a</span> <a href="../../core_kernel/Core_kernel__Type_equal/Id/index.html#type-t">Core.Univ_map.Key.t</a> <span>&#45;&gt;</span> <span class="type-var">'a</span> option</code></dt><dd><p><code>find_local key</code> returns the value associated to <code>key</code> in the current execution context.</p></dd></dl><dl><dt class="spec value" id="val-schedule'"><a href="#val-schedule'" class="anchor"></a><code><span class="keyword">val</span> schedule' : ((unit <span>&#45;&gt;</span> <span class="type-var">'a</span> <a href="../../async_kernel/Async_kernel/Deferred/index.html#type-t">Async_unix__.Import.Deferred.t</a>) <span>&#45;&gt;</span> <span class="type-var">'a</span> <a href="../../async_kernel/Async_kernel/Deferred/index.html#type-t">Async_unix__.Import.Deferred.t</a>) <a href="index.html#type-with_options">with_options</a></code></dt><dd><p>Just like <code>within'</code>, but instead of running the thunk right now, adds it to the Async queue to be run with other Async jobs.</p></dd></dl><dl><dt class="spec value" id="val-schedule"><a href="#val-schedule" class="anchor"></a><code><span class="keyword">val</span> schedule : ((unit <span>&#45;&gt;</span> unit) <span>&#45;&gt;</span> unit) <a href="index.html#type-with_options">with_options</a></code></dt><dd><p>Just like <code>schedule'</code>, but doesn't require the thunk to return a deferred.</p></dd></dl><dl><dt class="spec value" id="val-preserve_execution_context"><a href="#val-preserve_execution_context" class="anchor"></a><code><span class="keyword">val</span> preserve_execution_context : (<span class="type-var">'a</span> <span>&#45;&gt;</span> unit) <span>&#45;&gt;</span> (<span class="type-var">'a</span> <span>&#45;&gt;</span> unit) <a href="../../base/Base/Staged/index.html#type-t">Core.Staged.t</a></code></dt><dd><p><code>preserve_execution_context t f</code> saves the current execution context and returns a function <code>g</code> such that <code>g a</code> runs <code>f a</code> in the saved execution context. <code>g a</code> becomes determined when <code>f a</code> becomes determined.</p></dd></dl><dl><dt class="spec value" id="val-preserve_execution_context'"><a href="#val-preserve_execution_context'" class="anchor"></a><code><span class="keyword">val</span> preserve_execution_context' : (<span class="type-var">'a</span> <span>&#45;&gt;</span> <span class="type-var">'b</span> <a href="../../async_kernel/Async_kernel/Deferred/index.html#type-t">Async_unix__.Import.Deferred.t</a>) <span>&#45;&gt;</span> (<span class="type-var">'a</span> <span>&#45;&gt;</span> <span class="type-var">'b</span> <a href="../../async_kernel/Async_kernel/Deferred/index.html#type-t">Async_unix__.Import.Deferred.t</a>) <a href="../../base/Base/Staged/index.html#type-t">Core.Staged.t</a></code></dt><dt class="spec value" id="val-cycle_start"><a href="#val-cycle_start" class="anchor"></a><code><span class="keyword">val</span> cycle_start : unit <span>&#45;&gt;</span> <a href="../../core/Core__/Core_time_float/index.html#type-t">Core.Time.t</a></code></dt><dd><p><code>cycle_start ()</code> returns the result of <code>Time.now ()</code> called at the beginning of cycle.</p></dd></dl><dl><dt class="spec value" id="val-cycle_start_ns"><a href="#val-cycle_start_ns" class="anchor"></a><code><span class="keyword">val</span> cycle_start_ns : unit <span>&#45;&gt;</span> <a href="../../core/Core__/Core_time_ns/index.html#type-t">Core.Time_ns.t</a></code></dt><dt class="spec value" id="val-cycle_times"><a href="#val-cycle_times" class="anchor"></a><code><span class="keyword">val</span> cycle_times : unit <span>&#45;&gt;</span> <a href="../../core_kernel/Core_kernel__Time_intf/module-type-S/Time/Span/index.html#type-t">Core.Time.Span.t</a> <a href="../../async_kernel/Async_kernel__/Async_stream/index.html#type-t">Async_unix__.Import.Stream.t</a></code></dt><dd><p><code>cycle_times ()</code> returns a stream that is extended with an element at the start of each Async cycle, with the amount of time that the previous cycle took, as determined by calls to <code>Time.now</code> at the beginning and end of the cycle.</p></dd></dl><dl><dt class="spec value" id="val-cycle_times_ns"><a href="#val-cycle_times_ns" class="anchor"></a><code><span class="keyword">val</span> cycle_times_ns : unit <span>&#45;&gt;</span> <a href="../../core/Core__Core_time_ns/Span/index.html#type-t">Core.Time_ns.Span.t</a> <a href="../../async_kernel/Async_kernel__/Async_stream/index.html#type-t">Async_unix__.Import.Stream.t</a></code></dt><dt class="spec value" id="val-long_cycles"><a href="#val-long_cycles" class="anchor"></a><code><span class="keyword">val</span> long_cycles : at_least:<a href="../../core/Core__Core_time_ns/Span/index.html#type-t">Core.Time_ns.Span.t</a> <span>&#45;&gt;</span> <a href="../../core/Core__Core_time_ns/Span/index.html#type-t">Core.Time_ns.Span.t</a> <a href="../../async_kernel/Async_kernel__/Async_stream/index.html#type-t">Async_unix__.Import.Stream.t</a></code></dt><dd><p><code>long_cycles ~at_least</code> returns a stream of cycles whose duration is at least <code>at_least</code>. <code>long_cycles</code> is more efficient than <code>cycle_times</code> because it only allocates a stream entry when there is a long cycle, rather than on every cycle.</p></dd></dl><dl><dt class="spec value" id="val-report_long_cycle_times"><a href="#val-report_long_cycle_times" class="anchor"></a><code><span class="keyword">val</span> report_long_cycle_times : ?&#8288;cutoff:<a href="../../core_kernel/Core_kernel__Time_intf/module-type-S/Time/Span/index.html#type-t">Core.Time.Span.t</a> <span>&#45;&gt;</span> unit <span>&#45;&gt;</span> unit</code></dt><dd><p><code>report_long_cycle_times ?cutoff ()</code> sets up something that will print a warning to stderr whenever there is an Async cycle that is too long, as specified by <code>cutoff</code>, whose default is 1s.</p></dd></dl><dl><dt class="spec value" id="val-cycle_count"><a href="#val-cycle_count" class="anchor"></a><code><span class="keyword">val</span> cycle_count : unit <span>&#45;&gt;</span> int</code></dt><dd><p><code>cycle_count ()</code> returns the total number of Async cycles that have happened.</p></dd></dl><dl><dt class="spec value" id="val-event_precision"><a href="#val-event_precision" class="anchor"></a><code><span class="keyword">val</span> event_precision : unit <span>&#45;&gt;</span> <a href="../../core_kernel/Core_kernel__Time_intf/module-type-S/Time/Span/index.html#type-t">Core.Time.Span.t</a></code></dt><dd><p>The <code>alarm_precision</code> of the timing-wheel used to implement Async's <code>Clock</code>.</p></dd></dl><dl><dt class="spec value" id="val-event_precision_ns"><a href="#val-event_precision_ns" class="anchor"></a><code><span class="keyword">val</span> event_precision_ns : unit <span>&#45;&gt;</span> <a href="../../core/Core__Core_time_ns/Span/index.html#type-t">Core.Time_ns.Span.t</a></code></dt><dt class="spec value" id="val-force_current_cycle_to_end"><a href="#val-force_current_cycle_to_end" class="anchor"></a><code><span class="keyword">val</span> force_current_cycle_to_end : unit <span>&#45;&gt;</span> unit</code></dt><dd><p><code>force_current_cycle_to_end ()</code> causes no more normal priority jobs to run in the current cycle, and for the end-of-cycle jobs (i.e., writes) to run, and then for the cycle to end.</p></dd></dl><dl><dt class="spec value" id="val-is_running"><a href="#val-is_running" class="anchor"></a><code><span class="keyword">val</span> is_running : unit <span>&#45;&gt;</span> bool</code></dt><dd><p><code>is_running ()</code> returns true if the scheduler has been started.</p></dd></dl><dl><dt class="spec value" id="val-set_max_num_jobs_per_priority_per_cycle"><a href="#val-set_max_num_jobs_per_priority_per_cycle" class="anchor"></a><code><span class="keyword">val</span> set_max_num_jobs_per_priority_per_cycle : int <span>&#45;&gt;</span> unit</code></dt><dd><p><code>set_max_num_jobs_per_priority_per_cycle int</code> sets the maximum number of jobs that will be done at each priority within each Async cycle. The default is <code>500</code>. <code>max_num_jobs_per_priority_per_cycle</code> retrieves the current value.</p></dd></dl><dl><dt class="spec value" id="val-max_num_jobs_per_priority_per_cycle"><a href="#val-max_num_jobs_per_priority_per_cycle" class="anchor"></a><code><span class="keyword">val</span> max_num_jobs_per_priority_per_cycle : unit <span>&#45;&gt;</span> int</code></dt><dt class="spec value" id="val-set_max_inter_cycle_timeout"><a href="#val-set_max_inter_cycle_timeout" class="anchor"></a><code><span class="keyword">val</span> set_max_inter_cycle_timeout : <a href="../../core_kernel/Core_kernel__Time_intf/module-type-S/Time/Span/index.html#type-t">Core.Time.Span.t</a> <span>&#45;&gt;</span> unit</code></dt><dd><p><code>set_max_inter_cycle_timeout span</code> sets the maximum amount of time the scheduler will remain blocked (on epoll or select) between cycles.</p></dd></dl><dl><dt class="spec value" id="val-set_check_invariants"><a href="#val-set_check_invariants" class="anchor"></a><code><span class="keyword">val</span> set_check_invariants : bool <span>&#45;&gt;</span> unit</code></dt><dd><p><code>set_check_invariants do_check</code> sets whether Async should check invariants of its internal data structures. <code>set_check_invariants true</code> can substantially slow down your program.</p></dd></dl><dl><dt class="spec value" id="val-set_detect_invalid_access_from_thread"><a href="#val-set_detect_invalid_access_from_thread" class="anchor"></a><code><span class="keyword">val</span> set_detect_invalid_access_from_thread : bool <span>&#45;&gt;</span> unit</code></dt><dd><p><code>set_detect_invalid_access_from_thread do_check</code> sets whether Async routines should check if they are being accessed from some thread other than the thread currently holding the Async lock, which is not allowed and can lead to very confusing behavior.</p></dd></dl><dl><dt class="spec value" id="val-set_record_backtraces"><a href="#val-set_record_backtraces" class="anchor"></a><code><span class="keyword">val</span> set_record_backtraces : bool <span>&#45;&gt;</span> unit</code></dt><dd><p><code>set_record_backtraces do_record</code> sets whether Async should keep in the execution context the history of stack backtraces (obtained via <code>Backtrace.get</code>) that led to the current job. If an Async job has an unhandled exception, this backtrace history will be recorded in the exception. In particular the history will appear in an unhandled exception that reaches the main monitor. This can have a substantial performance impact, both in running time and space usage.</p></dd></dl><dl><dt class="spec type" id="type-folder"><a href="#type-folder" class="anchor"></a><code><span class="keyword">type</span> 'b folder</code><code> = </code><code>{</code><table class="record"><tr id="type-folder.folder" class="anchored"><td class="def field"><a href="#type-folder.folder" class="anchor"></a><code>folder : a. <span class="type-var">'b</span> <span>&#45;&gt;</span> <a href="index.html#type-t">t</a> <span>&#45;&gt;</span> (<a href="index.html#type-t">t</a>, <span class="type-var">'a</span>) <a href="../../base/Base/Field/index.html#type-t">Core.Field.t</a> <span>&#45;&gt;</span> <span class="type-var">'b</span>;</code></td></tr></table><code>}</code></dt></dl><dl><dt class="spec value" id="val-fold_fields"><a href="#val-fold_fields" class="anchor"></a><code><span class="keyword">val</span> fold_fields : init:<span class="type-var">'b</span> <span>&#45;&gt;</span> <span class="type-var">'b</span> <a href="index.html#type-folder">folder</a> <span>&#45;&gt;</span> <span class="type-var">'b</span></code></dt><dd><p><code>fold_fields ~init folder</code> folds <code>folder</code> over each field in the scheduler. The fields themselves are not exposed -- <code>folder</code> must be a polymorphic function that can work on any field. So, it's only useful for generic operations, e.g., getting the size of each field.</p></dd></dl><dl><dt class="spec value" id="val-is_ready_to_initialize"><a href="#val-is_ready_to_initialize" class="anchor"></a><code><span class="keyword">val</span> is_ready_to_initialize : unit <span>&#45;&gt;</span> bool</code></dt><dt class="spec value" id="val-reset_in_forked_process"><a href="#val-reset_in_forked_process" class="anchor"></a><code><span class="keyword">val</span> reset_in_forked_process : unit <span>&#45;&gt;</span> unit</code></dt><dd><p>If a process that has already created, but not started, the Async scheduler would like to fork, and would like the child to have a clean Async, i.e., not inherit any of the Async work that was done in the parent, it can call <code>reset_in_forked_process</code> at the start of execution in the child process. After that, the child can do Async stuff and then start the Async scheduler.</p></dd></dl><dl><dt class="spec value" id="val-make_async_unusable"><a href="#val-make_async_unusable" class="anchor"></a><code><span class="keyword">val</span> make_async_unusable : unit <span>&#45;&gt;</span> unit</code></dt><dd><p><code>make_async_unusable ()</code> makes subsequent attempts to use the Async scheduler raise. One use case for <code>make_async_unusable</code> is if you fork from a process already running the Async scheduler, and want to run non-Async OCaml code in the child process, with the guarantee that the child process does not use Async.</p></dd></dl><dl><dt class="spec value" id="val-add_busy_poller"><a href="#val-add_busy_poller" class="anchor"></a><code><span class="keyword">val</span> add_busy_poller : (unit <span>&#45;&gt;</span> [ `Continue_polling | `Stop_polling of <span class="type-var">'a</span> ]) <span>&#45;&gt;</span> <span class="type-var">'a</span> <a href="../../async_kernel/Async_kernel/Deferred/index.html#type-t">Async_unix__.Import.Deferred.t</a></code></dt><dd><p>Async supports &quot;busy polling&quot;, which runs a thread that busy loops running user-supplied polling functions. The busy-loop thread is distinct from Async's scheduler thread.</p><p>Busy polling is useful for a situation like a shared-memory ringbuffer being used for IPC. One can poll the ringbuffer with a busy poller, and then when data is detected, fill some ivar that causes Async code to handle the data.</p><p><code>add_busy_poller poll</code> adds <code>poll</code> to the busy loop. <code>poll</code> will be called continuously, once per iteration of the busy loop, until it returns <code>`Stop_polling a</code> at which point the result of <code>add_busy_poller</code> will become determined. <code>poll</code> will hold the Async lock while running, so it is fine to do ordinary Async operations, e.g., fill an ivar. The busy loop will run an ordinary Async cycle if any of the pollers add jobs.</p><p><code>poll</code> will run in the monitor in effect when <code>add_busy_poller</code> was called; exceptions raised by <code>poll</code> will be sent asynchronously to that monitor. If <code>poll</code> raises, it will still be run on subsequent iterations of the busy loop.</p></dd></dl><dl><dt class="spec value" id="val-handle_thread_pool_stuck"><a href="#val-handle_thread_pool_stuck" class="anchor"></a><code><span class="keyword">val</span> handle_thread_pool_stuck : (stuck_for:<a href="../../core/Core__Core_time_ns/Span/index.html#type-t">Core.Time_ns.Span.t</a> <span>&#45;&gt;</span> unit) <span>&#45;&gt;</span> unit</code></dt><dd><p><code>handle_thread_pool_stuck f</code> causes <code>f</code> to run whenever Async detects its thread pool is stuck (i.e., hasn't completed a job for over a second and has work waiting to start). Async checks every second. By default, if the thread pool has been stuck for less than 60s, Async will <code>eprintf</code> a message. If more than 60s, Async will send an exception to the main monitor, which will abort the program unless there is a custom handler for the main monitor.</p><p>Calling <code>handle_thread_pool_stuck</code> replaces whatever behavior was previously there.</p></dd></dl><dl><dt class="spec value" id="val-default_handle_thread_pool_stuck"><a href="#val-default_handle_thread_pool_stuck" class="anchor"></a><code><span class="keyword">val</span> default_handle_thread_pool_stuck : stuck_for:<a href="../../core/Core__Core_time_ns/Span/index.html#type-t">Core.Time_ns.Span.t</a> <span>&#45;&gt;</span> unit</code></dt><dt class="spec value" id="val-yield"><a href="#val-yield" class="anchor"></a><code><span class="keyword">val</span> yield : unit <span>&#45;&gt;</span> unit <a href="../../async_kernel/Async_kernel/Deferred/index.html#type-t">Async_unix__.Import.Deferred.t</a></code></dt><dd><p><code>yield ()</code> returns a deferred that becomes determined after the current cycle completes. This can be useful to improve fairness by <code>yield</code>ing within a computation to give other jobs a chance to run.</p></dd></dl><dl><dt class="spec value" id="val-yield_until_no_jobs_remain"><a href="#val-yield_until_no_jobs_remain" class="anchor"></a><code><span class="keyword">val</span> yield_until_no_jobs_remain : unit <span>&#45;&gt;</span> unit <a href="../../async_kernel/Async_kernel/Deferred/index.html#type-t">Async_unix__.Import.Deferred.t</a></code></dt><dd><p><code>yield_until_no_jobs_remain ()</code> returns a deferred that becomes determined the next time Async's job queue is empty. This is useful in tests when one needs to wait for the completion of all the jobs based on what's in the queue, when those jobs might create other jobs -- without depending on I/O or the passage of wall-clock time.</p></dd></dl><dl><dt class="spec value" id="val-yield_every"><a href="#val-yield_every" class="anchor"></a><code><span class="keyword">val</span> yield_every : n:int <span>&#45;&gt;</span> (unit <span>&#45;&gt;</span> unit <a href="../../async_kernel/Async_kernel/Deferred/index.html#type-t">Async_unix__.Import.Deferred.t</a>) <a href="../../base/Base/Staged/index.html#type-t">Core.Staged.t</a></code></dt><dd><p><code>yield_every ~n</code> returns a function that will act as <code>yield</code> every <code>n</code> calls and as <code>return ()</code> the rest of the time. This is useful for improving fairness in circumstances where you don't have good control of the batch size, but can insert a deferred into every iteration.</p><p><code>yield_every</code> raises if <code>n &lt;= 0</code>.</p></dd></dl><dl><dt class="spec value" id="val-time_spent_waiting_for_io"><a href="#val-time_spent_waiting_for_io" class="anchor"></a><code><span class="keyword">val</span> time_spent_waiting_for_io : unit <span>&#45;&gt;</span> <a href="../../core/Core__Core_time_ns/Span/index.html#type-t">Core.Time_ns.Span.t</a></code></dt><dd><p><code>time_spent_waiting_for_io ()</code> returns the amount of time that the Async scheduler has spent in calls to <code>epoll_wait</code> (or <code>select</code>) since the start of the program.</p></dd></dl><dl><dt class="spec value" id="val-set_min_inter_cycle_timeout"><a href="#val-set_min_inter_cycle_timeout" class="anchor"></a><code><span class="keyword">val</span> set_min_inter_cycle_timeout : <a href="../../core/Core__Core_time_ns/Span/index.html#type-t">Core.Time_ns.Span.t</a> <span>&#45;&gt;</span> unit</code></dt><dd><p><code>set_min_inter_cycle_timeout</code> sets the minimum timeout that the scheduler will pass to the OS when it checks for I/O between cycles. The minimum is zero by default. Setting it to a nonzero value is used to increase thread fairness between the scheduler and other threads. A plausible setting is 10us. This can also be set via the <code>ASYNC_CONFIG</code> environment variable.</p></dd></dl><dl><dt class="spec value" id="val-set_may_sleep_for_thread_fairness"><a href="#val-set_may_sleep_for_thread_fairness" class="anchor"></a><code><span class="keyword">val</span> set_may_sleep_for_thread_fairness : bool <span>&#45;&gt;</span> unit</code></dt><dd><p><code>may_sleep_for_thread_fairness</code> controls whether the scheduler calls <code>nanosleep</code> each cycle to give other threads a chance to run. The default is <code>false</code>.</p></dd></dl><dl><dt class="spec value" id="val-num_jobs_run"><a href="#val-num_jobs_run" class="anchor"></a><code><span class="keyword">val</span> num_jobs_run : unit <span>&#45;&gt;</span> int</code></dt><dd><p><code>num_jobs_run ()</code> returns the number of jobs that have been run since starting. The returned value includes the currently running job.</p></dd></dl><dl><dt class="spec value" id="val-num_pending_jobs"><a href="#val-num_pending_jobs" class="anchor"></a><code><span class="keyword">val</span> num_pending_jobs : unit <span>&#45;&gt;</span> int</code></dt><dd><p><code>num_pending_jobs</code> returns the number of jobs that are queued to run by the scheduler.</p></dd></dl><div class="spec module" id="module-Expert"><a href="#module-Expert" class="anchor"></a><code><span class="keyword">module</span> <a href="Expert/index.html">Expert</a> : <span class="keyword">sig</span> ... <span class="keyword">end</span></code></div></div></body></html>