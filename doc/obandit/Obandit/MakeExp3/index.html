<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml"><head><title>MakeExp3 (obandit.Obandit.MakeExp3)</title><link rel="stylesheet" href="../../../_odoc-theme/odoc.css"/><meta charset="utf-8"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><script src="../../../highlight.pack.js"></script><script>hljs.initHighlightingOnLoad();</script></head><body><div class="content"><header><nav><a href="../index.html">Up</a> â€“ <a href="../../index.html">obandit</a> &#x00BB; <a href="../index.html">Obandit</a> &#x00BB; MakeExp3</nav><h1>Module <code>Obandit.MakeExp3</code></h1><p>The Exp3 Bandit for adversarial regret minimization with a parametrizable learning rate.</p></header><h3 class="heading">Parameters</h3><ul><li><code><a href="argument-1-P/index.html">P</a> : <a href="../index.html#module-type-RateBanditParam">RateBanditParam</a></code></li></ul><h3 class="heading">Signature</h3><dl><dt class="spec type" id="type-bandit"><a href="#type-bandit" class="anchor"></a><code><span class="keyword">type</span> bandit</code><code> = <a href="../index.html#type-banditPolicy">banditPolicy</a></code></dt><dd><p>The internal data structure of the bandit algorithm.</p></dd></dl><dl><dt class="spec value" id="val-initialBandit"><a href="#val-initialBandit" class="anchor"></a><code><span class="keyword">val</span> initialBandit : <a href="index.html#type-bandit">bandit</a></code></dt><dd><p>The internal data structure of the bandit algorithm.</p><p>The initial state of the bandit algorithm.</p></dd></dl><dl><dt class="spec value" id="val-step"><a href="#val-step" class="anchor"></a><code><span class="keyword">val</span> step : <a href="index.html#type-bandit">bandit</a> <span>&#45;&gt;</span> float <span>&#45;&gt;</span> int * <a href="index.html#type-bandit">bandit</a></code></dt><dd><p>The initial state of the bandit algorithm.</p><p><code>step r</code> advances the bandit game one step, where <code>r</code> is the reward for the last action. The result of this call is the next action, encoded as an integer in $ \{ 0, \cdots , K-1 \} $, and the new state of the bandit. The reward range depends on the bandit algorithm in use and the first reward provided to the algorithm is discarded.</p></dd></dl></div></body></html>